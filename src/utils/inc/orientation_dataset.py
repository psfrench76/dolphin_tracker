import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image, UnidentifiedImageError
from pathlib import Path
from .settings import settings
import random
import hashlib
import time
from collections import OrderedDict
import sys
import multiprocessing
import torchvision.transforms.functional as tf

class DolphinOrientationDataset(Dataset):
    shared_cache = None
    shared_cache_size = None
    cache_stats = None
    shared_cache_keys = None
    """
    Annotations can be passed in as a YOLO output file. If it is left blank, it will load annotations from the dataset.
    images_index_file must also be passed. This is a file generated by track.py that contains the image file names,
    because frame numbers are not guaranteed to be unique.
    """
    def __init__(self, dataset_root_dir, annotations=None, images_index_file=None, augment=False, imgsz=244, cache_limit_mb=100):
        self.dataset_root_path = Path(dataset_root_dir)
        self.augment = augment
        self.cache_limit_mb = cache_limit_mb

        if DolphinOrientationDataset.shared_cache is None:
            manager = multiprocessing.Manager()
            DolphinOrientationDataset.shared_cache = manager.dict()
            DolphinOrientationDataset.shared_cache_keys = manager.list()
            DolphinOrientationDataset.shared_cache_size = manager.Value('i', 0)  # Shared integer for cache size
            DolphinOrientationDataset.cache_stats = manager.dict({'hits': 0, 'misses': 0})

        print(f"Initializing dataset {dataset_root_dir} with cache size {cache_limit_mb} MB")

    #TODO: I think a lot of indexing is happening when it doesn't need to, i.e. things are coming from YOLO. Investigate.
        self.image_dir = self.dataset_root_path / settings['images_dir']
        image_files = sorted([f for f in self.image_dir.iterdir() if f.suffix in settings['image_file_extensions']])
        self.images_index = {}
        for image_file in image_files:
            self.images_index[image_file.stem] = image_file

        self.image_library = {}
        self.imgsz = imgsz

        self.orientations_index = {}
        self.orientations_dir = self.dataset_root_path / settings['orientations_dir']

        self.tracks_index = {}
        self.tracks_dir = self.dataset_root_path / settings['tracks_dir']

        if self.tracks_dir.exists():
            tracks_files = sorted(self.tracks_dir.iterdir())
            for tracks_file in tracks_files:
                self.tracks_index[tracks_file.stem] = tracks_file

        if annotations:
            annotations_path = Path(annotations)
            images_index_path = Path(images_index_file)
            if not annotations_path.exists() or not images_index_path.exists():
                raise FileNotFoundError(f"Annotations or images index file not found: {annotations_path}, {images_index_path}")
            self.annotations = self._convert_annotations_from_yolo(annotations_path, images_index_path)

        else:
            orientations_files = sorted(self.orientations_dir.iterdir())
            for orientations_file in orientations_files:
                self.orientations_index[orientations_file.stem] = orientations_file

            self.annotations = self._load_annotations_from_dataset_dir(self.dataset_root_path)

        self.transform = transforms.Compose([
            transforms.Resize(size=self.imgsz-1, max_size=self.imgsz),
            # transforms.Lambda(lambda img: tf.pad(img, (0, 0, imgsz - img.size[0], imgsz - img.size[1]), fill=0)),  # Black padding

            # transforms.Pad(padding=lambda img: ((imgsz - img.size[0]) // 2,  # L/R padding
            #                                     (imgsz - img.size[1]) // 2,  # U/D padding
            #     ), fill=0,  # Black padding
            # ),
            transforms.CenterCrop(self.imgsz),  # Center crop to square (handles padding)

            # transforms.Resize((self.imgsz, self.imgsz)),
            transforms.ToTensor(),
            # Normalize to mean and standard deviations of RGB values for ImageNet
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):

        cache = DolphinOrientationDataset.shared_cache
        annotation = self.annotations[idx]
        img_scale = None

        if idx in cache:
            image, orientation, track = self._cache_load(idx)

        else:
            orientation = annotation['orientation']
            track = annotation['track']
            img_path = annotation['image']
            bbox = annotation['bbox']
            image = self._load_image_from_library(img_path, bbox, 'yolo')

            if orientation is None:
                orientation = torch.empty(0, 0, dtype=torch.float32)
            else:
                orientation = torch.tensor(orientation, dtype=torch.float32)

            img_max_dim = max(image.size)
            img_scale = self.imgsz / img_max_dim
            orientation = orientation * img_scale

            # print(f"Rescaling image to: {self.imgsz}, Orientation: {orientation}")

            image_tensor = self.transform(image)
            image.close()
            image = image_tensor



            self._cache_insert(image, orientation, track, idx)

        # print(f"\nAugmenting image {annotation}")
        # print(f"Scaled image by {img_scale}")
        # self._save_image_from_tensor(image, f"before_{idx}.jpg")
        # print(f"Image dimensions: {image.size()}, Orientation: {orientation}, Track: {track}, Index: {idx}")

        if self.augment:
            image, orientation = self._apply_augmentation(image, orientation)

        # print(f"Done processing image {annotation}")
        # self._save_image_from_tensor(image, f"after_{idx}.jpg")
        # print(f"Image dimensions: {image.size()}, Orientation: {orientation}, Track: {track}, Index: {idx}")

        # print(f"Image dimensions: {image.size}, Orientation: {orientation}, Track: {track}, Index: {idx}")


        return image, orientation, track, idx

    def _save_image_from_tensor(self, image_tensor, save_path):
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        denormalized_image = image_tensor * std + mean  # Reverse normalization
        pil_image = tf.to_pil_image(denormalized_image.clamp(0, 1))  # Clamp values to [0, 1] for valid image range
        pil_image.save(save_path)

    def _cache_insert(self, image, orientation, track, idx):
        cache = DolphinOrientationDataset.shared_cache
        cache_size = DolphinOrientationDataset.shared_cache_size
        cache_keys = DolphinOrientationDataset.shared_cache_keys
        cache_stats = DolphinOrientationDataset.cache_stats

        new_item = (image.clone(), orientation.clone(), track)
        new_item_size = sys.getsizeof(image) + sys.getsizeof(orientation) + sys.getsizeof(track)
        while cache_size.value + new_item_size > self.cache_limit_mb * 1024 * 1024:
            evict_key = cache_keys.pop(0)
            evicted_item = cache.pop(evict_key)
            evicted_item_size = sum(sys.getsizeof(x) for x in evicted_item)
            cache_size.value -= evicted_item_size
        cache[idx] = new_item
        cache_keys.append(idx)
        cache_size.value += new_item_size

        cache_stats['misses'] += 1
        # print(f"Cache size: {cache_size.value / 1024 / 1024:.2f} MB, Cache hits: {cache_stats['hits']}, Cache misses: {cache_stats['misses']}")


    def _cache_load(self, idx):
        cache_keys = DolphinOrientationDataset.shared_cache_keys
        cache = DolphinOrientationDataset.shared_cache
        cache_stats = DolphinOrientationDataset.cache_stats

        image, orientation, track = cache[idx]

        cache_keys.remove(idx)
        cache_keys.append(idx)

        cache_stats['hits'] += 1

        return image.clone(), orientation.clone(), track

    def get_cache_size_mb(self):
        cache_size = DolphinOrientationDataset.shared_cache_size
        return cache_size.value / 1024 / 1024

    def get_image_path(self, idx):
        return self.annotations[idx]['image']

    def get_ground_truth_label(self, idx):
        return self.annotations[idx]['image'], self.annotations[idx]['orientation'], self.annotations[idx]['track'], idx

    def _load_image_from_library(self, image_path, bbox, bbox_format):
        bbox_serialized = str((bbox, bbox_format)).encode('utf-8')
        bbox_hash = hashlib.sha256(bbox_serialized).hexdigest()

        library_image_name = f"{image_path.stem}_{bbox_hash}{image_path.suffix}"
        library_image_path = self.dataset_root_path / settings['orientations_library_dir'] / library_image_name
        library_image_path.parent.mkdir(parents=True, exist_ok=True)

        # image = None

        # if image_path not in self.image_library:
        #     self.image_library[image_path] = {}

        # if bbox_hash in self.image_library[image_path]:
        #     image = self.image_library[image_path][bbox_hash]
        #     print(f"Image found in library: {image_path} with bbox {bbox_hash}, loading from internal cache")
        #     return image
        if library_image_path.exists():
            image = None
            for i in range (0, 5):
                try:
                    # print(f"Trying to open image {library_image_path}, attempt {i+1}")
                    image = Image.open(library_image_path).convert("RGB")
                    break
                except UnidentifiedImageError:
                    if library_image_path.stat().st_size == 0:
                        print(f"Image {library_image_path} is empty, reloading from source and resaving...")
                        cropped_image = load_cropped_image(image_path, bbox, bbox_format)
                        return cropped_image

                    time.sleep(0.5)

            if image is None:
                raise UnidentifiedImageError(f"Tried 5 times and failed; Could not open image {library_image_path}")

            # image = Image.open(library_image_path).convert("RGB")
            # self.image_library[image_path][bbox_hash] = image
            # print(f"Image found: {image_path} with bbox {bbox_hash}, loading from file {library_image_path}")
            return image
        else:
            cropped_image = load_cropped_image(image_path, bbox, bbox_format)
            # print(f"No image found: {image_path} with bbox {bbox_hash}, saving image to file {library_image_path}")
            cropped_image.save(library_image_path)
            # self.image_library[image_path][bbox_hash] = cropped_image
            # image = cropped_image
            return cropped_image

        # for i in range (0, 5):
        #     try:
        #         print(f"Trying to open image {library_image_path}, attempt {i+1}")
        #         image = Image.open(library_image_path).convert("RGB")
        #     except UnidentifiedImageError:
        #         time.sleep(0.5)
        #
        # if image is None:
        #     raise UnidentifiedImageError(f"Tried 5 times and failed; Could not open image {library_image_path}")
        #
        # return image

    def _apply_augmentation(self, image, orientation):
        # Randomly apply L/R or U/D flips
        lr_chance = 0.5
        ud_chance = 0.5

        if random.random() < lr_chance:  # 50% chance for L/R flip
            # print(f"Before applying X augmentation: {image.size()}, Orientation: {orientation}")
            image = tf.hflip(image)
            # # image = image.transpose(Image.FLIP_LEFT_RIGHT)
            # bbox = (1.0 - x, y, w, h)
            orientation[0] = -orientation[0]  # Flip x-component of orientation
            # print(f"After applying X augmentation: {image.size()}, Orientation: {orientation}")

        if random.random() < ud_chance:  # 50% chance for U/D flip
            # print(f"Before applying Y augmentation: {image.size()}, Orientation: {orientation}")
            image = tf.vflip(image)
            # image = image.transpose(Image.FLIP_TOP_BOTTOM)
            # bbox = (x, 1.0 - y, w, h)  # Flip y-center
            orientation[1] = -orientation[1]  # Flip y-component of orientation
            # print(f"After applying Y augmentation: {image.size()}, Orientation: {orientation}")



        return image, orientation

    def _load_annotations_from_dataset_dir(self, dataset_root_path):
        annotations = []
        labels_dir_path = dataset_root_path / settings['labels_dir']
        for label_file_path in sorted(labels_dir_path.glob('*.txt')):

            if label_file_path.stem not in self.orientations_index:
                raise FileNotFoundError(f"Orientation file for {label_file_path.stem} not found.")
            else:
                orientation_file_path = self.orientations_index[label_file_path.stem]

            image_path = self.images_index[label_file_path.stem]

            orientations = {int(label_index): [float(x), float(y)] for label_index, x, y in
                            [val.split() for val in orientation_file_path.read_text().splitlines()]}

            for label_index, line in enumerate(label_file_path.read_text().splitlines()):

                if label_index not in orientations:
                    continue

                if label_file_path.stem not in self.tracks_index:
                    raise FileNotFoundError(f"Track file for {label_file_path.stem} not found.")
                else:
                    track_file_path = self.tracks_index[label_file_path.stem]

                tracks = track_file_path.read_text().splitlines()
                track = int(tracks[label_index])

                _, x_center, y_center, width, height = map(float, line.split())
                bbox = (x_center, y_center, width, height)

                annotations.append({'image': image_path, 'bbox': bbox, 'orientation': orientations[label_index], 'track': track})

        return annotations


    def _convert_annotations_from_yolo(self, yolo_file, images_index_file):
        annotations = []
        with open(yolo_file, 'r') as file:
            with open(images_index_file, 'r') as index_file:
                for line, image_path in zip(file, index_file):
                    track, x_center, y_center, width, height = map(float, line.strip().split(',')[1:6])
                    track = int(track)
                    image_path = Path(image_path.strip())

                    bbox = (x_center, y_center, width, height)
                    orientation = None

                    annotations.append({'image': image_path, 'bbox': bbox, 'orientation': orientation, 'track': track})

        return annotations


def load_cropped_image(image_path, bbox, bbox_format):
    image_path = Path(image_path)
    image = Image.open(image_path).convert("RGB")
    width, height = image.size

    if bbox_format == 'yolo':
        x_center, y_center, box_width, box_height = bbox
        x_center *= width
        y_center *= height
        box_width *= width
        box_height *= height

        x_top_left = x_center - box_width / 2
        y_top_left = y_center - box_height / 2
        x_bottom_right = x_center + box_width / 2
        y_bottom_right = y_center + box_height / 2
    elif bbox_format == 'pixels':
        x_top_left, y_top_left, x_bottom_right, y_bottom_right = bbox
    else:
        raise ValueError("bbox_format must be either 'yolo' or 'pixels'")

    cropped_image = image.crop((x_top_left, y_top_left, x_bottom_right, y_bottom_right))
    return cropped_image