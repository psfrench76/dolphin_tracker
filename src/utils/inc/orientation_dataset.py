import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
from pathlib import Path
from .settings import settings
import random
import hashlib
from collections import OrderedDict
import sys
import multiprocessing

class DolphinOrientationDataset(Dataset):
    shared_cache = None
    shared_cache_size = None
    cache_stats = None
    shared_cache_keys = None
    """
    Annotations can be passed in as a YOLO output file. If it is left blank, it will load annotations from the dataset.
    images_index_file must also be passed. This is a file generated by track.py that contains the image file names,
    because frame numbers are not guaranteed to be unique.
    """
    def __init__(self, dataset_root_dir, annotations=None, images_index_file=None, augment=False, imgsz=244, cache_limit_mb=100):
        self.dataset_root_path = Path(dataset_root_dir)
        self.augment = augment
        self.cache_limit_mb = cache_limit_mb

        if DolphinOrientationDataset.shared_cache is None:
            manager = multiprocessing.Manager()
            DolphinOrientationDataset.shared_cache = manager.dict()
            DolphinOrientationDataset.shared_cache_keys = manager.list()
            DolphinOrientationDataset.shared_cache_size = manager.Value('i', 0)  # Shared integer for cache size
            DolphinOrientationDataset.cache_stats = manager.dict({'hits': 0, 'misses': 0})

        print(f"Initializing dataset {dataset_root_dir} with cache size {cache_limit_mb} MB")

    #TODO: I think a lot of indexing is happening when it doesn't need to, i.e. things are coming from YOLO. Investigate.
        self.image_dir = self.dataset_root_path / settings['images_dir']
        image_files = sorted([f for f in self.image_dir.iterdir() if f.suffix in settings['image_file_extensions']])
        self.images_index = {}
        for image_file in image_files:
            self.images_index[image_file.stem] = image_file

        self.image_library = {}

        self.orientations_index = {}
        self.orientations_dir = self.dataset_root_path / settings['orientations_dir']

        self.tracks_index = {}
        self.tracks_dir = self.dataset_root_path / settings['tracks_dir']

        if self.tracks_dir.exists():
            tracks_files = sorted(self.tracks_dir.iterdir())
            for tracks_file in tracks_files:
                self.tracks_index[tracks_file.stem] = tracks_file

        if annotations:
            annotations_path = Path(annotations)
            images_index_path = Path(images_index_file)
            if not annotations_path.exists() or not images_index_path.exists():
                raise FileNotFoundError(f"Annotations or images index file not found: {annotations_path}, {images_index_path}")
            self.annotations = self._convert_annotations_from_yolo(annotations_path, images_index_path)

        else:
            orientations_files = sorted(self.orientations_dir.iterdir())
            for orientations_file in orientations_files:
                self.orientations_index[orientations_file.stem] = orientations_file

            self.annotations = self._load_annotations_from_dataset_dir(self.dataset_root_path)

        self.transform = transforms.Compose([
            transforms.Resize((imgsz, imgsz)),
            transforms.ToTensor(),
            # Normalize to mean and standard deviations of RGB values for ImageNet
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):

        cache = DolphinOrientationDataset.shared_cache
        cache_size = DolphinOrientationDataset.shared_cache_size
        cache_stats = DolphinOrientationDataset.cache_stats
        cache_keys = DolphinOrientationDataset.shared_cache_keys

        if idx in cache:
            # Move accessed item to the end to mark it as recently used
            image, orientation, track = cache[idx]

            cache_keys.remove(idx)
            cache_keys.append(idx)

            cache_stats['hits'] += 1
        else:
            annotation = self.annotations[idx]
            orientation = annotation['orientation']
            track = annotation['track']
            img_path = annotation['image']
            bbox = annotation['bbox']
            image = self._load_image_from_library(img_path, bbox, 'yolo')

            if orientation is None:
                orientation = torch.empty(0, 0, dtype=torch.float32)
            else:
                orientation = torch.tensor(orientation, dtype=torch.float32)

            if self.transform:
                image = self.transform(image)

            new_item = (image, orientation, track)
            new_item_size = sys.getsizeof(image) + sys.getsizeof(orientation) + sys.getsizeof(track)
            while cache_size.value + new_item_size > self.cache_limit_mb * 1024 * 1024:
                evict_key = cache_keys.pop(0)
                evicted_item = cache.pop(evict_key)
                evicted_item_size = sum(sys.getsizeof(x) for x in evicted_item)
                cache_size.value -= evicted_item_size
            cache[idx] = new_item
            cache_keys.append(idx)
            cache_size.value += new_item_size
            cache_stats['misses'] += 1

        if self.augment:
            image, orientation, bbox = self._apply_augmentation(image, orientation, self.annotations[idx]['bbox'])

        return image, orientation, track, idx

    def get_image_path(self, idx):
        return self.annotations[idx]['image']

    def get_ground_truth_label(self, idx):
        return self.annotations[idx]['image'], self.annotations[idx]['orientation'], self.annotations[idx]['track'], idx

    def _load_image_from_library(self, image_path, bbox, bbox_format):
        bbox_serialized = str((bbox, bbox_format)).encode('utf-8')
        bbox_hash = hashlib.sha256(bbox_serialized).hexdigest()

        library_image_name = f"{image_path.stem}_{bbox_hash}{image_path.suffix}"
        library_image_path = self.dataset_root_path / settings['orientations_library_dir'] / library_image_name

        if image_path not in self.image_library:
            self.image_library[image_path] = {}

        if bbox_hash in self.image_library[image_path]:
            library_image_path = self.image_library[image_path][bbox_hash]
        elif library_image_path.exists():
            self.image_library[image_path][bbox_hash] = library_image_path
        else:
            cropped_image = load_cropped_image(image_path, bbox, bbox_format)
            library_image_path.parent.mkdir(parents=True, exist_ok=True)

            cropped_image.save(library_image_path)
            self.image_library[image_path][bbox_hash] = library_image_path

        return Image.open(library_image_path).convert("RGB")

    def _apply_augmentation(self, image, orientation, bbox):
        # Randomly apply L/R or U/D flips
        if random.random() < 0.5:  # 50% chance for L/R flip
            image = image.transpose(Image.FLIP_LEFT_RIGHT)
            bbox[0] = 1.0 - bbox[0]  # Flip x-center
            orientation[0] = -orientation[0]  # Flip x-component of orientation

        if random.random() < 0.5:  # 50% chance for U/D flip
            image = image.transpose(Image.FLIP_TOP_BOTTOM)
            bbox[1] = 1.0 - bbox[1]  # Flip y-center
            orientation[1] = -orientation[1]  # Flip y-component of orientation

        return image, orientation, bbox

    def _load_annotations_from_dataset_dir(self, dataset_root_path):
        annotations = []
        labels_dir_path = dataset_root_path / settings['labels_dir']
        for label_file_path in sorted(labels_dir_path.glob('*.txt')):

            if label_file_path.stem not in self.orientations_index:
                raise FileNotFoundError(f"Orientation file for {label_file_path.stem} not found.")
            else:
                orientation_file_path = self.orientations_index[label_file_path.stem]

            if label_file_path.stem not in self.tracks_index:
                raise FileNotFoundError(f"Track file for {label_file_path.stem} not found.")
            else:
                track_file_path = self.tracks_index[label_file_path.stem]

            image_path = self.images_index[label_file_path.stem]

            orientations = {int(label_index): [float(x), float(y)] for label_index, x, y in
                            [val.split() for val in orientation_file_path.read_text().splitlines()]}

            for label_index, (line, track) in enumerate(zip(label_file_path.read_text().splitlines(),
                                                          track_file_path.read_text().splitlines())):
                if label_index not in orientations:
                    continue

                track = int(track)
                _, x_center, y_center, width, height = map(float, line.split())
                bbox = (x_center, y_center, width, height)

                annotations.append({'image': image_path, 'bbox': bbox, 'orientation': orientations[label_index], 'track': track})

        return annotations


    def _convert_annotations_from_yolo(self, yolo_file, images_index_file):
        annotations = []
        with open(yolo_file, 'r') as file:
            with open(images_index_file, 'r') as index_file:
                for line, image_path in zip(file, index_file):
                    track, x_center, y_center, width, height = map(float, line.strip().split(',')[1:6])
                    track = int(track)
                    image_path = Path(image_path.strip())

                    bbox = (x_center, y_center, width, height)
                    orientation = None

                    annotations.append({'image': image_path, 'bbox': bbox, 'orientation': orientation, 'track': track})

        return annotations


def load_cropped_image(image_path, bbox, bbox_format):
    image_path = Path(image_path)
    image = Image.open(image_path).convert("RGB")
    width, height = image.size

    if bbox_format == 'yolo':
        x_center, y_center, box_width, box_height = bbox
        x_center *= width
        y_center *= height
        box_width *= width
        box_height *= height

        x_top_left = x_center - box_width / 2
        y_top_left = y_center - box_height / 2
        x_bottom_right = x_center + box_width / 2
        y_bottom_right = y_center + box_height / 2
    elif bbox_format == 'pixels':
        x_top_left, y_top_left, x_bottom_right, y_bottom_right = bbox
    else:
        raise ValueError("bbox_format must be either 'yolo' or 'pixels'")

    cropped_image = image.crop((x_top_left, y_top_left, x_bottom_right, y_bottom_right))
    return cropped_image